{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached version of scalawebsocket_2.10-0.1.1.jar\n",
      "Using cached version of spark-cloudant-2.0.0-s_2.11.jar\n",
      "Using cached version of async-http-client-1.8.0.jar\n",
      "Using cached version of scalalogging-log4j_2.10-1.1.0.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar http://central.maven.org/maven2/eu/piotrbuda/scalawebsocket_2.10/0.1.1/scalawebsocket_2.10-0.1.1.jar\n",
    "%AddJar http://dl.bintray.com/spark-packages/maven/cloudant-labs/spark-cloudant/2.0.0-s_2.11/spark-cloudant-2.0.0-s_2.11.jar\n",
    "%AddJar http://central.maven.org/maven2/com/ning/async-http-client/1.8.0/async-http-client-1.8.0.jar\n",
    "%AddJar http://central.maven.org/maven2/com/typesafe/scalalogging-log4j_2.10/1.1.0/scalalogging-log4j_2.10-1.1.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cloudant.password,004f4adfb5a46442c3a5e37f6d62eec477c9f3d96781d45f612bfba6afd4b013)\n",
      "(spark.eventLog.enabled,true)\n",
      "(spark.deploy.resourceScheduler.factory,org.apache.spark.deploy.master.EGOResourceSchedulerFactory)\n",
      "(cloudant.host,83a4d5b3-b0cd-4206-a1dd-b8ede559ef88-bluemix.cloudant.com)\n",
      "(spark.r.command,/usr/local/src/bluemix_jupyter_bundle.v31/R/bin/Rscript)\n",
      "(spark.externalBlockStore.folderName,spark-a0c6ee27-7bf5-451c-b9dd-e4838a0adff4)\n",
      "(spark.sql.tungsten.enabled,false)\n",
      "(spark.eventLog.dir,/gpfs/fs01/user/sb8e-31a08100070616-ac6107fb81b2/events)\n",
      "(spark.app.id,local-1482439757746)\n",
      "(spark.driver.port,35787)\n",
      "(spark.jars,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/JSON4J-1.1.0.0.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/commons-io-2.4.jar,file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb8e-31a08100070616-ac6107fb81b2/data/libs/scala-2.10/*,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/httpmime.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/stocator-1.0.7.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/ia-stan-classifiers-201611220127.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/jackson-mapper-asl-1.6.2.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/joda-convert-1.6.jar,file:/usr/local/src/event-stream/spark-1.6.0/libs/messagehub.login-1.0.0.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/spark-assembly-1.6.0-hadoop2.6.0.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/wink-json4j-provider-1.4.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/parquet-hadoop-bundle-1.8.1.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/BigInsightsWesternNERStandard.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/log4j-1.2.17.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/httpclient-4.5.2.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/aws/jackson-annotations-2.4.4.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/commons-codec-1.9.jar,file:/usr/local/src/dataconnector-db2/db2jcc4-10.5.0.6.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/aws/jackson-databind-2.4.4.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/dataworksml-scala-1.0.0.0-201610252355.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/ASBServer/apps/lib/iis/*/*,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/branded_jdbc/lib/*,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/commons-lang-2.6.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/htmlparser-2.1.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/spark-avro_2.10-2.0.1.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/system-t-runtime-4.6-biginsights.jar,file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb8e-31a08100070616-ac6107fb81b2/notebook/spark-config/spark160master/,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jdbc/lib/*,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/kernel-api_2.10-0.1.5-SNAPSHOT.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jars/scapi-fileformat.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/play-datacommons_2.10-2.3.6.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/commons-logging-1.1.1.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/cglib-2.2.1-v20090111.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/hadoop-core-1.1.0.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/avro-1.7.7.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/ngwb.oms_client-11.5.1.0.665.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/spss-scala-modelviewer-3.2.0.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/datanucleus-api-jdo-3.2.6.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/ego/spark-ego_2.10-1.6.0.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/joss-0.9.14.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/hadoop-auth-2.7.0.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/commons-net-3.1.jar,file:/usr/local/src/dataconnector-s3/spark-1.6.0/libs/aws-java-sdk-1.7.4.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/spring-core-4.1.1.RELEASE.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/jackson-core-asl-1.6.2.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/slf4j-simple-1.6.1.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/antlr4-runtime-4.5.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/ngp-catalog-0.1.19-201609191657.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/mysql-connector-java-6.0.2.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/ingestion-scala-0.1.551-201612042232.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/httpclient-4.3.3.jar,file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb8e-31a08100070616-ac6107fb81b2/data/libs/*,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/slf4j-api-1.6.1.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/ego/spark-launcher_2.10-1.6.0.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/hamcrest-core-1.1.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/univocity-parsers-1.5.1.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jars/FileConnector.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/communication_2.10-0.1.5-SNAPSHOT.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jars/scapi-util.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/httpcore-4.2.4.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/commons-logging-1.1.3.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/xmlbeans-2.6.0.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/junit-4.10.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/transformation-scala-0.1.476-201610112143.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/project-notebook-integration-assembly-0.10.1.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/aspera.tokengen.3.1.0.65813.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/poi-ooxml-3.14.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/commons-codec-1.6.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/guice-3.0.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jars/scapi-dw.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/icu4j-53.1.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/log4j-1.2.17.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/aopalliance-1.0.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/dataquality-201611220127.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/guava-14.0.1.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/protocol_2.10-0.1.5-SNAPSHOT.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/commons-csv-1.1.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/play-functional_2.10-2.3.6.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/jcl-over-slf4j-1.7.2.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/jackson-mapper-asl-1.9.7.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/pipeline-scala-0.1.416-201611100637.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/joda-time-2.3.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/systemml-0.11.0-incubating.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/datanucleus-rdbms-3.2.9.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/scala-stm_2.10-0.7.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/jackson-core-asl-1.9.7.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/slf4j-log4j12-1.7.2.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/asm-3.2.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/stan_api-201611220127.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/jackson-core-asl-1.9.13.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/aws/aws-java-sdk-1.7.3.jar,file:/usr/local/src/event-stream/spark-1.6.0/libs/event-stream-1.6.0.2.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/libs/SparkRelationProvider1.6-12.0.0.1-20161215-0026.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/simple-regex-4.6.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/avro-1.8.0.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/slf4j-api-1.7.2.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/parquet-avro-1.8.1.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/util-scala-0.1.37-201611302306.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/ego/gson-2.2.4.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/aws/jackson-core-2.4.4.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/ego/Java-WebSocket-1.3.0.jar,file:/usr/local/src/dataconnector-cloudant/spark-cloudant-1.6.3-125.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/javax.inject-1.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/commons-configuration-1.7.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/poi-ooxml-schemas-3.14.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/config/,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/ego/spark-network-shuffle_2.10-1.6.0.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/spss-spark-assembly-4.1.0.0-201611082150.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/tika-app-1.10.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/tika-parsers-1.10.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/jeromq-0.3.4.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/tika-core-1.10.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/guava-14.0.1.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/commons-codec-1.10.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/ego/guava-14.0.1.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/gson.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/ivy-2.4.0.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/commons-lang-2.6.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/ngwb.oms_api-11.5.1.0.665.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/json-simple-1.1.1.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/htmllexer-2.1.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/play-json_2.10-2.3.6.jar,file:/usr/local/src/dataconnector-s3/spark-1.6.0/libs/hadoop-aws-2.6.0.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/commons-io-2.3.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/httpcore-4.3.2.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/jopt-simple-4.6.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/connectors/*/*,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/FaspStreamSDK/lib/*,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jars/JISPlugins/*,file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb8e-31a08100070616-ac6107fb81b2/data/libs/,file:/usr/local/src/analytic-libs/spark-1.6.0/ngwb.oms_common-11.5.1.0.665.jar,file:/usr/local/src/event-stream/spark-1.6.0/libs/kafka_2.10-0.10.0.1.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/hadoop-common-2.7.0.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/macros_2.10-0.1.5-SNAPSHOT.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/iis_util.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/spark-kernel-brunel-all-1.1.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/quasiquotes_2.10-2.0.1.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/poi-3.14.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/play-iteratees_2.10-2.3.6.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/kernel_2.10-0.1.5-SNAPSHOT.jar,file:/usr/local/src/analytic-libs/spark-1.6.0/spark-csv_2.10-1.5.0.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/jackson-mapper-asl-1.9.13.jar,file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/datanucleus-core-3.2.10.jar,file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/httpcore-4.4.5.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/commons-collections-3.2.2.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jars/scapi.jar,file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/faspstream.jar,file:/gpfs/fs01/user/sb8e-31a08100070616-ac6107fb81b2/notebook/downloads/scalawebsocket_2.10-0.1.1.jar,file:/gpfs/fs01/user/sb8e-31a08100070616-ac6107fb81b2/notebook/downloads/spark-cloudant-2.0.0-s_2.11.jar,file:/gpfs/fs01/user/sb8e-31a08100070616-ac6107fb81b2/notebook/downloads/async-http-client-1.8.0.jar,file:/gpfs/fs01/user/sb8e-31a08100070616-ac6107fb81b2/notebook/downloads/scalalogging-log4j_2.10-1.1.0.jar)\n",
      "(cloudant.username,83a4d5b3-b0cd-4206-a1dd-b8ede559ef88-bluemix)\n",
      "(spark.master,local[*])\n",
      "(spark.worker.ui.retainedExecutors,0)\n",
      "(spark.shuffle.service.port,7340)\n",
      "(spark.ui.retainedJobs,0)\n",
      "(spark.shuffle.service.enabled,true)\n",
      "(spark.driver.allowMultipleContexts,true)\n",
      "(spark.executor.extraJavaOptions,-Djava.security.egd=file:/dev/./urandom)\n",
      "(spark.executor.id,driver)\n",
      "(spark.port.maxRetries,512)\n",
      "(spark.logConf,true)\n",
      "(spark.history.fs.logDirectory,/gpfs/fs01/user/sb8e-31a08100070616-ac6107fb81b2/events)\n",
      "(spark.repl.class.uri,http://10.143.133.172:40388)\n",
      "(spark.driver.host,10.143.133.172)\n",
      "(spark.app.name,IBM Spark Kernel)\n",
      "(spark.ui.enabled,false)\n",
      "(spark.task.maxFailures,10)\n",
      "(spark.sql.unsafe.enabled,false)\n",
      "(spark.ui.retainedStages,0)\n",
      "(spark.driver.maxResultSize,1210M)\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.streaming.{Time, Seconds, StreamingContext}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "val conf = sc.getConf\n",
    "conf.set(\"cloudant.host\",\"83a4d5b3-b0cd-4206-a1dd-b8ede559ef88-bluemix.cloudant.com\")\n",
    "conf.set(\"cloudant.username\", \"83a4d5b3-b0cd-4206-a1dd-b8ede559ef88-bluemix\")\n",
    "conf.set(\"cloudant.password\",\"004f4adfb5a46442c3a5e37f6d62eec477c9f3d96781d45f612bfba6afd4b013\")\n",
    "\n",
    "conf.setJars(ClassLoader.getSystemClassLoader.asInstanceOf[java.net.URLClassLoader].getURLs.map(_.toString).toSet.toSeq ++ kernel.interpreter.classLoader.asInstanceOf[java.net.URLClassLoader].getURLs.map(_.toString).toSeq)\n",
    "conf.set(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "conf.set(\"spark.master\",\"local[*]\")\n",
    "val scCloudant = new SparkContext(conf)\n",
    "scCloudant.getConf.getAll.foreach(println)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeToDatabse(databaseName: String, df: DataFrame) = {\n",
    "    df.write.format(\"com.cloudant.spark\").save(databaseName)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.storage.StorageLevel\n",
    "import scalawebsocket.WebSocket\n",
    "import org.apache.spark.streaming.receiver.Receiver\n",
    "import org.apache.spark.Logging\n",
    "\n",
    "import org.json4s._\n",
    "import org.json4s.DefaultFormats\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "\n",
    "case class MeetupEvent(event_id: String,event_name: Option[String],event_url: Option[String],time: Long)\n",
    "case class MeetupGroupTopics(topic_name: Option[String],urlkey: Option[String])\n",
    "case class MeetupGroup( group_id: Long, group_name: String,group_city: Option[String],group_country: Option[String], group_state: Option[String],  group_urlname: Option[String], group_lat: Option[String],group_lon: Option[String],group_topics: List[MeetupGroupTopics]) \n",
    "case class MeetupMember( member_id: Long, member_name: Option[String],other_services: Option[String],photo: Option[String])\n",
    "case class MeetupVenue(venue_id: Long, venue_name: Option[String],lat: Option[String], lon: Option[String])\n",
    "case class MeetupRsvp(rsvp_id: Long,response: String,guests: Int, mtime: Long, visibility : String, event: MeetupEvent, group: MeetupGroup, member: MeetupMember, venue: MeetupVenue)\n",
    "\n",
    "\n",
    "class WebSocketReceiver(url: String, storageLevel: StorageLevel) extends Receiver[MeetupRsvp](storageLevel) with Logging {\n",
    "  private var webSocket: WebSocket = _\n",
    "  \n",
    "  def onStart() {\n",
    "    try{\n",
    "      log(\"Connecting to WebSocket: \" + url)\n",
    "      val newWebSocket = WebSocket().open(url).onTextMessage({ msg: String => parseJson(msg) })\n",
    "      setWebSocket(newWebSocket)\n",
    "      log(\"Connected to: WebSocket\" + url)\n",
    "    } catch {\n",
    "      case e: Exception => restart(\"Error starting WebSocket stream\", e)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def onStop() {\n",
    "    setWebSocket(null)\n",
    "    log(\"WebSocket receiver stopped\")\n",
    "  }\n",
    "\n",
    "  private def setWebSocket(newWebSocket: WebSocket) = synchronized {\n",
    "    if (webSocket != null) {\n",
    "      webSocket.shutdown()\n",
    "    }\n",
    "    webSocket = newWebSocket\n",
    "  }\n",
    "\n",
    "  private def parseJson(jsonStr: String): Unit =\n",
    "  {\n",
    "    try {\n",
    "      implicit lazy val formats = DefaultFormats\n",
    "      val json = parse(jsonStr)\n",
    "      val rsvp = json.extract[MeetupRsvp]\n",
    "      store(rsvp)\n",
    "    } catch {\n",
    "       case e: Exception => e.getMessage()\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use connectorVersion=1.6.3, dbName=meetup_group, indexName=null, viewName=null,jsonstore.rdd.partitions=5, + jsonstore.rdd.maxInPartition=-1,jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=900000,bulkSize=20, schemaSampleSize=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 18, localhost): java.lang.RuntimeException: Database meetup_group: nothing was saved because the number of records was 0!\n",
       "\tat com.cloudant.spark.common.JsonStoreDataAccess.saveAll(JsonStoreDataAccess.scala:187)\n",
       "\tat com.cloudant.spark.CloudantReadWriteRelation$$anonfun$3.apply(DefaultSource.scala:86)\n",
       "\tat com.cloudant.spark.CloudantReadWriteRelation$$anonfun$3.apply(DefaultSource.scala:84)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$33.apply(RDD.scala:920)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1863)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1863)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:233)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
       "\tat java.lang.Thread.run(Thread.java:785)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "java.lang.Thread.getStackTrace(Thread.java:1117)\n",
       "org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1850)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1863)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\n",
       "org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:920)\n",
       "org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:918)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:918)\n",
       "com.cloudant.spark.CloudantReadWriteRelation.insert(DefaultSource.scala:84)\n",
       "com.cloudant.spark.DefaultSource.createRelation(DefaultSource.scala:130)\n",
       "com.cloudant.spark.DefaultSource.createRelation(DefaultSource.scala:94)\n",
       "org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:222)\n",
       "org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)\n",
       "org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:139)\n",
       "$line56.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.writeToDatabse(<console>:36)\n",
       "$line80.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$persistStream$1.apply(<console>:76)\n",
       "$line80.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$persistStream$1.apply(<console>:69)\n",
       "org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:50)\n",
       "org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)\n",
       "org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)\n",
       "org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:426)\n",
       "org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:49)\n",
       "org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)\n",
       "org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)\n",
       "scala.util.Try$.apply(Try.scala:161)\n",
       "org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
       "org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:224)\n",
       "org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)\n",
       "org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)\n",
       "scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n",
       "org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:223)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
       "java.lang.Thread.run(Thread.java:785)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use connectorVersion=1.6.3, dbName=meetup_group, indexName=null, viewName=null,jsonstore.rdd.partitions=5, + jsonstore.rdd.maxInPartition=-1,jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=900000,bulkSize=20, schemaSampleSize=1\n",
      "Use connectorVersion=1.6.3, dbName=meetup_group, indexName=null, viewName=null,jsonstore.rdd.partitions=5, + jsonstore.rdd.maxInPartition=-1,jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=900000,bulkSize=20, schemaSampleSize=1\n",
      "+--------+--------------------+----------+-------------+-----------+--------------------+---------+---------+--------------------+\n",
      "|group_id|          group_name|group_city|group_country|group_state|       group_urlname|group_lat|group_lon|        group_topics|\n",
      "+--------+--------------------+----------+-------------+-----------+--------------------+---------+---------+--------------------+\n",
      "| 1693770|Houston Single Zo...|   Houston|           us|         TX|HoustonSingleZoomers|    29.75|   -95.49|[[Singles,singles...|\n",
      "+--------+--------------------+----------+-------------+-----------+--------------------+---------+---------+--------------------+\n",
      "\n",
      "Use connectorVersion=1.6.3, dbName=meetup_group, indexName=null, viewName=null,jsonstore.rdd.partitions=5, + jsonstore.rdd.maxInPartition=-1,jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=900000,bulkSize=20, schemaSampleSize=1\n",
      "Use connectorVersion=1.6.3, dbName=meetup_group, indexName=null, viewName=null,jsonstore.rdd.partitions=5, + jsonstore.rdd.maxInPartition=-1,jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=900000,bulkSize=20, schemaSampleSize=1\n",
      "Use connectorVersion=1.6.3, dbName=meetup_group, indexName=null, viewName=null,jsonstore.rdd.partitions=5, + jsonstore.rdd.maxInPartition=-1,jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=900000,bulkSize=20, schemaSampleSize=1\n",
      "+--------+--------------------+-----------+-------------+-----------+------------------+---------+---------+--------------------+\n",
      "|group_id|          group_name| group_city|group_country|group_state|     group_urlname|group_lat|group_lon|        group_topics|\n",
      "+--------+--------------------+-----------+-------------+-----------+------------------+---------+---------+--------------------+\n",
      "|  931700|The San Antonio N...|San Antonio|           us|         TX|sanaturehounds-com|     29.5|    -98.4|[[Dogs,dogs], [Do...|\n",
      "+--------+--------------------+-----------+-------------+-----------+------------------+---------+---------+--------------------+\n",
      "\n",
      "Use connectorVersion=1.6.3, dbName=meetup_group, indexName=null, viewName=null,jsonstore.rdd.partitions=5, + jsonstore.rdd.maxInPartition=-1,jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=900000,bulkSize=20, schemaSampleSize=1\n"
     ]
    }
   ],
   "source": [
    "def persistStream(conf: SparkConf) = {\n",
    "    val ssc = new StreamingContext(conf, Seconds(10))\n",
    "    val stream = ssc.receiverStream[MeetupRsvp](new WebSocketReceiver(\"ws://stream.meetup.com/2/rsvps\", StorageLevel.MEMORY_ONLY_SER))\n",
    "    stream.foreachRDD((rdd: RDD[MeetupRsvp], time: Time) => {\n",
    "      val sqlContext = new SQLContext(rdd.sparkContext)  \n",
    "      import sqlContext.implicits._\n",
    "      val df = rdd.map(data => {\n",
    "        data.group\n",
    "      }).filter(_.group_state.getOrElse(\"\").equals(\"TX\")).toDF()\n",
    "      if(df.collect().length > 0) {\n",
    "        writeToDatabse(\"meetup_group\", df)\n",
    "        df.show()\n",
    "      }\n",
    "    })\n",
    "    ssc.start()\n",
    "    ssc.awaitTerminationOrTimeout(60000)\n",
    "}\n",
    "\n",
    "persistStream(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10 with Spark 1.6",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}